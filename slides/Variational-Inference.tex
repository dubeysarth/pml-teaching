\documentclass[handout]{beamer}

\usetheme[progressbar=frametitle]{metropolis}
\usepackage{appendixnumberbeamer}
\usepackage{booktabs}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\definecolor{metropolisblue}{RGB}{39, 59, 94}



% Begin document
\begin{document}

% Title page
\title{Variational Inference}
\author{Nipun Batra}
\date{\today}
\institute{IIT Gandhinagar}
\maketitle

% Section 1
\section{Introduction}

\begin{frame}{Bayesian ML: Recap}
    \begin{itemize}
        \item We assume a prior distribution over the parameters of the model given as $P(\theta)$
        \item We assume a likelihood function $P(D|\theta)$
        \item We use Bayes' rule to find the posterior distribution of the parameters given the data: $P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}$
        \item Typically, we can not compute the posterior distribution analytically as the denominator is intractable
    \end{itemize}
    
\end{frame}

\begin{frame}{Bayesian ML: Methods}
    \begin{columns}[T]
        \begin{column}{.33\textwidth}
            \textbf{Laplace Approximation}
            
            \vspace{0.2cm}
            Approximates the posterior with a Gaussian distribution parameterized by $\Psi = (\mu, \Sigma)$.
            {\tiny
            \[
                q_{\Psi}(\theta) = \mathcal{N}(\mu, \Sigma)
            \]}
            
            \vspace{0.2cm}
            where $\mu$ is the mode of the posterior and $\Sigma$ is the negative inverse Hessian of the log joint distribution evaluated at $\theta_{\text{MAP}}$.
        \end{column}
        
        \begin{column}{.33\textwidth}
            \textbf{MCMC (Markov Chain Monte Carlo)}
            
            \vspace{0.2cm}
            Generates samples from the posterior distribution by constructing a Markov chain.
            {\tiny
            \[
                P(\theta|D) \propto P(D|\theta)P(\theta)
            \]}
        \end{column}
        
        \begin{column}{.33\textwidth}
            \textbf{Variational Inference}
            
            \vspace{0.5cm}
            Poses posterior inference as an optimization problem. The approximating distribution is parameterized by $\Psi$.
            {\tiny
            \[
                \Psi^* = \arg\min_{\Psi} \text{KL}(q_{\Psi}(\theta)||P(\theta|D))
            \]}
        \end{column}
    \end{columns}
\end{frame}




\begin{frame}{KL Divergence}
    \begin{itemize}
        \item KL divergence is a measure of dissimilarity between two distributions.
        \item It is defined as: $\text{KL}(q||p) = \int q(\theta) \log \frac{q(\theta)}{p(\theta)} d\theta$
    \end{itemize}
    
\end{frame}

\begin{frame}{Exercise}
    Compute the KL divergence between two Gaussian distributions $q(\theta) = \mathcal{N}(\mu_1, \sigma_1^2)$ and $p(\theta) = \mathcal{N}(\mu_2, \sigma_2^2)$.

    \pause The answer is: $\frac{1}{2} \left( \log \frac{\sigma_2^2}{\sigma_1^2} + \frac{\sigma_1^2 + (\mu_1 - \mu_2)^2}{\sigma_2^2} - 1 \right)$
    
\end{frame}

\begin{frame}
    Notebook demo
\end{frame}

\begin{frame}{Optimizing}
    
\end{frame}

\begin{frame}
    Notebook demo
\end{frame}

\begin{frame}{Monte Carlo Sampling}
    
\end{frame}

\begin{frame}
    Notebook demo
\end{frame}

\begin{frame}{Repameterization Trick}
    
\end{frame}

\begin{frame}
    Notebook demo
\end{frame}

\begin{frame}{ELBO}
\end{frame}
    
\begin{frame}{Worked out example: Coin Toss}
   
\end{frame}

\begin{frame}{Worked out example: Linear Regression}
    
\end{frame}

\begin{frame}{Worked out example: Neural Networks}
\end{frame}

    


\end{document}